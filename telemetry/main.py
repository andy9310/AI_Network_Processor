from fastapi import FastAPI, HTTPException
from pathlib import Path
import random, json, os, torch, textwrap
from unsloth import FastLanguageModel
from dotenv import load_dotenv
from collect import collect_link_traffic, fetch_generic_counters_legacy
load_dotenv()
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â¶ è¼‰å…¥æ¨¡å‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MAX_SEQ_LEN  = 2048
MODEL_NAME   = "taide/Llama-3.1-TAIDE-LX-8B-Chat"

print("ğŸš€ Loading modelâ€¦ (åªè¼‰ä¸€æ¬¡)")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name     = MODEL_NAME,
    max_seq_length = MAX_SEQ_LEN,
    load_in_4bit   = True,                # ğŸ”§ 4-bit é‡åŒ–â†’ 8 GB VRAM å¤ ç”¨
    token          = os.getenv("HUGGINGFACE_TOKEN", ""),
)                 # ğŸ”§ ä¸€æ¬¡æ€§æŠŠæ•´å€‹æ¨¡å‹æ¬ä¸Š GPU
print("âœ… Model on", model.device)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â· System prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RULES_PATH = Path("rules.txt")
rules_text = RULES_PATH.read_text(encoding="utf-8")

SYSTEM_PROMPT = textwrap.dedent(f"""{rules_text.strip()}

ä½ æ˜¯ Cisco IOS XR ç¶²ç®¡åŠ©æ‰‹ï¼Œåªè¼¸å‡º RESTCONF curl æŒ‡ä»¤èˆ‡å°æ‡‰ config JSONï¼Œ
ç¦æ­¢åŠ ä¸Šä»»ä½•è§£é‡‹æˆ–å¤šé¤˜æ–‡å­—ã€‚""")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â¸ Telemetry ç”¢ç”Ÿå™¨ (ç•¥)â€¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LINKS â€¦ generate_random_traffic() èˆ‡ collector() å®Œå…¨ç…§èˆŠ
# (æ­¤è™•çœç•¥ï¼Œä¿æŒåŸä¾†å‡½å¼ä¸è®Š)
LINKS = [
    "S1-S2", "S1-S3", "S1-S4", "S1-S9",
    "S2-S1", "S2-S4", "S2-S9",
    "S3-S1", "S3-S4", "S3-S9",
    "S4-S1", "S4-S2", "S4-S3", "S4-S5", "S4-S6", "S4-S7",
    "S4-S8", "S4-S9", "S4-S10", "S4-S11", "S4-S15",
    "S5-S4", "S5-S9",
    "S6-S4", "S6-S15",
    "S7-S4", "S7-S9",
    "S8-S4", "S8-S9",
    "S9-S1", "S9-S2", "S9-S3", "S9-S4", "S9-S5",
    "S9-S7", "S9-S8", "S9-S10", "S9-S15",
    "S10-S4", "S10-S9", "S10-S12", "S10-S13",
    "S10-S14", "S10-S15", "S10-S16", "S10-S17",
    "S11-S4", "S11-S15",
    "S12-S10", "S12-S15",
    "S13-S10", "S13-S15",
    "S14-S10", "S14-S15",
    "S15-S4", "S15-S6", "S15-S9", "S15-S10", "S15-S11",
    "S15-S12", "S15-S13", "S15-S14", "S15-S16", "S15-S17",
    "S16-S10", "S16-S15",
    "S17-S10", "S17-S15",
]

def generate_random_traffic(links=LINKS, min_traffic=1, max_traffic=1_000):
    traffic = {}
    for link in links:
        src, dst = link.split("-")
        fwd_key, rev_key = f"{src}-{dst}", f"{dst}-{src}"
        if fwd_key not in traffic:
            traffic[fwd_key] = random.randint(min_traffic, max_traffic)
        if rev_key not in traffic:
            traffic[rev_key] = random.randint(min_traffic, max_traffic)
    return traffic
def default_collector():
    """default Telemetry ç”¢ç”Ÿå™¨ã€‚"""
    return generate_random_traffic()

def collector():
    """å®Œæ•´ Telemetry ç”¢ç”Ÿå™¨ã€‚"""
    return collect_link_traffic(LINKS)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â¹ LLM æ¨è«– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def llm_inference(links_to_close=None):
    if links_to_close is None:
        links_to_close = ["L1", "L3", "L6"]

    telemetry_json = json.dumps(collector(), ensure_ascii=False)
    user_prompt = (
        f"ä»¥ä¸‹æ˜¯å³æ™‚æµé‡è³‡æ–™ï¼ˆJSONï¼‰ï¼š\n{telemetry_json}\n\n"
        f"è«‹æ ¹æ“šä¸Šè¿°è³‡æ–™ï¼Œé—œé–‰ {', '.join(links_to_close)}"
    )

    chat = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user",   "content": user_prompt},
    ]

    input_ids = tokenizer.apply_chat_template(
        chat,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
    ).to("cuda")                           # ğŸ”§ å¼µé‡ä¹Ÿæ¬åˆ° GPU

    with torch.no_grad():
        outputs = model.generate(
            input_ids       = input_ids,
            max_new_tokens  = 256,
            do_sample       = True,
            temperature     = 0.7,
            top_p           = 0.9,
        )

    generated = outputs[0, input_ids.shape[-1]:]
    return tokenizer.decode(generated, skip_special_tokens=True).strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âº FastAPI ç«¯é» (åŸæ¨£) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = FastAPI(
    title       = "XR Telemetry + LLM Demo",
    description = "ç”¢ç”ŸTelemetryï¼Œä¸¦ç”¨ UnsLoTH Llama-3.1 taide ç”¢ RESTCONF æŒ‡ä»¤",
    version     = "1.0.0",
)

@app.get("/telemetry")
async def get_telemetry():
    try:
        return default_collector()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/output")
async def get_output(links: str | None = None):
    try:
        result = llm_inference(links.split(",") if links else None)
        return {"result": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â» å•Ÿå‹• (ä¿æŒåŸæœ‰) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# if __name__ == "__main__":
#     import uvicorn, sys
#     uvicorn.run("main:app", host="0.0.0.0", port=8000,
#                 reload="--reload" in sys.argv)
