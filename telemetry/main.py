import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional
import json
import re, os, torch, textwrap, numpy as np
from pathlib import Path
from unsloth import FastLanguageModel
from dotenv import load_dotenv
from collect import collect_link_traffic, fetch_generic_counters_legacy
from rl_model import get_rl_manager
from rag_system import get_rag_system
from restconf_processor import process_predicted_links, build_shutdown_commands
load_dotenv()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ é…ç½®è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
USE_RAG = False  # Set to True to enable RAG, False to disable
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¼‰å…¥LLMæ¨¡å‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MAX_SEQ_LEN  = 2048
MODEL_NAME   = "taide/Llama-3.1-TAIDE-LX-8B-Chat"
print("Loading modelâ€¦ (åªè¼‰ä¸€æ¬¡)")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name     = MODEL_NAME,
    max_seq_length = MAX_SEQ_LEN,
    load_in_4bit   = True,                # ğŸ”§ 4-bit é‡åŒ–â†’ 8 GB VRAM å¤ ç”¨
    token          = os.getenv("HUGGINGFACE_TOKEN", ""),
)           
print("Model on", model.device)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¼‰å…¥RLæ¨¡å‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# åˆå§‹åŒ–RLæ¨¡å‹ç®¡ç†å™¨ (ä½¿ç”¨çœŸå¯¦è¨“ç·´å¥½çš„æ¨¡å‹)
rl_manager = get_rl_manager(use_mock=False)
print(f"ğŸ¤– RL Model Info: {rl_manager.get_model_info()}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¼‰å…¥RAGç³»çµ± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# åˆå§‹åŒ–RAGç³»çµ± (æ ¹æ“šé…ç½®æ±ºå®šæ˜¯å¦å•Ÿç”¨)
if USE_RAG:
    try:
        rag_system = get_rag_system("all-MiniLM-L6-v2")
        print("ğŸ“š RAG System initialized with free local embeddings")

        # å˜—è©¦è¼‰å…¥Guide.docxæ–‡æª”
        try:
            rag_system.load_documents("Guide.docx")
            print("âœ… Guide.docx loaded into RAG system")
        except Exception as e:
            print(f"âš ï¸ Failed to load Guide.docx: {e}")
            print("ğŸ“ You can load documents later using the /load-document endpoint")
    except Exception as e:
        print(f"âš ï¸ RAG System initialization failed: {e}")
        print("ğŸ“ RAG features will be disabled")
        rag_system = None
else:
    rag_system = None
    print("ğŸš« RAG System disabled by configuration (USE_RAG = False)")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â¸ System prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYSTEM_PROMPT = textwrap.dedent(f"""ä½ æ˜¯ Cisco IOS XR ç¶²ç®¡åŠ©æ‰‹ï¼Œä½ å°‡è¢«æä¾›ç¶²è·¯æµé‡è³‡æ–™ï¼Œè«‹åˆ†æè³‡æ–™å¾Œï¼Œè¼¸å‡ºåˆ†æçµæœï¼Œè«‹ç°¡çŸ­æ‰¼è¦ï¼Œä¸è¦è¼¸å‡ºæŒ‡ä»¤ï¼Œä¸è¶…é2000å­—å…ƒ""")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â„ Dynamic Links Generation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_dynamic_links():
    """Generate LINKS dynamically from topology data"""
    try:
        from collect import get_dynamic_interface_mapping
        
        # Get dynamic interface mapping which contains all link information
        interface_mapping = get_dynamic_interface_mapping()
        
        # Extract unique links from the mapping keys
        links = set()
        for link_name in interface_mapping.keys():
            links.add(link_name)
        
        # Convert to sorted list for consistency
        dynamic_links = sorted(list(links))
        print(f"ğŸ”— Generated {len(dynamic_links)} dynamic links from topology")
        return dynamic_links
        
    except Exception as e:
        print(f"âš ï¸  Error generating dynamic links: {e}")
        print("ğŸ”„ Falling back to static LINKS")
        return STATIC_LINKS

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â¹ Telemetry ç”¢ç”Ÿå™¨ (ç•¥)â€¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Static fallback LINKS (kept for backup)
STATIC_LINKS = [
    "S1-S2", "S1-S3", "S1-S4", "S1-S9",
    "S2-S1", "S2-S4", "S2-S9",
    "S3-S1", "S3-S4", "S3-S9",
    "S4-S1", "S4-S2", "S4-S3", "S4-S5", "S4-S6", "S4-S7",
    "S4-S8", "S4-S9", "S4-S10", "S4-S11", "S4-S15",
    "S5-S4", "S5-S9",
    "S6-S4", "S6-S15",
    "S7-S4", "S7-S9",
    "S8-S4", "S8-S9",
    "S9-S1", "S9-S2", "S9-S3", "S9-S4", "S9-S5",
    "S9-S7", "S9-S8", "S9-S10", "S9-S15",
    "S10-S4", "S10-S9", "S10-S12", "S10-S13",
    "S10-S14", "S10-S15", "S10-S16", "S10-S17",
    "S11-S4", "S11-S15",
    "S12-S10", "S12-S15",
    "S13-S10", "S13-S15",
    "S14-S10", "S14-S15",
    "S15-S4", "S15-S6", "S15-S9", "S15-S10", "S15-S11",
    "S15-S12", "S15-S13", "S15-S14", "S15-S16", "S15-S17",
    "S16-S10", "S16-S15",
    "S17-S10", "S17-S15",
]

# Generate dynamic links at startup
LINKS = get_dynamic_links()

def generate_random_traffic(links=LINKS, min_traffic=1, max_traffic=1_000):
    traffic = {}
    for link in links:
        src, dst = link.split("-")
        fwd_key, rev_key = f"{src}-{dst}", f"{dst}-{src}"
        if fwd_key not in traffic:
            traffic[fwd_key] = random.randint(min_traffic, max_traffic)
        if rev_key not in traffic:
            traffic[rev_key] = random.randint(min_traffic, max_traffic)
    return traffic
def default_collector():
    """default Telemetry ç”¢ç”Ÿå™¨ã€‚"""
    return generate_random_traffic()

def collector(use_rates=True, measurement_interval=10):
    """å®Œæ•´ Telemetry ç”¢ç”Ÿå™¨ - uses dynamic links from topology with real-time ratesã€‚
    
    Args:
        use_rates (bool): If True, collect traffic rates; if False, use cumulative counters
        measurement_interval (int): Seconds between measurements for rate calculation
    
    Returns:
        dict: Traffic data (rates in bytes/sec or cumulative bytes)
    """
    if use_rates:
        from collect import collect_traffic_rates
        return collect_traffic_rates(LINKS, measurement_interval=measurement_interval)
    else:
        return collect_link_traffic(LINKS)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âº RLæ¨¡å‹é æ¸¬å‡½æ•¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def predict_links_to_close_rl(telemetry_data: dict) -> list:
    """
    ä½¿ç”¨RLæ¨¡å‹é æ¸¬æ‡‰è©²é—œé–‰å“ªäº›link
    
    Args:
        telemetry_data: å³æ™‚æµé‡æ•¸æ“š
        
    Returns:
        list: æ‡‰è©²é—œé–‰çš„linkåˆ—è¡¨
    """
    try:
        return rl_manager.predict_links_to_close(telemetry_data)
    except Exception as e:
        print(f"âŒ Error in RL prediction: {e}")
        return ["L1", "L3", "L6"]  # é è¨­å€¼

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â» LLM æ¨è«– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def llm_inference(user_prompt: str = None, predicted_links: list = None):
    """
    LLM inference with RAG enhancement
    
    Args:
        user_prompt: Custom user prompt (optional)
        predicted_links: Pre-filtered links to close (optional, if None will predict using RL)
    """
    telemetry_data = collector() ## link utilization
    
    # Use provided filtered links or predict using RL
    if predicted_links is not None:
        links_to_close = predicted_links
        print(f"ğŸ”’ Using filtered links to close: {links_to_close}")
    else:
        links_to_close = predict_links_to_close_rl(telemetry_data)
        print(f"ğŸ¤– RL predicted links to close: {links_to_close}")
    
    # Calculate energy saving based on bidirectional link pairs
    def find_bidirectional_pairs_in_telemetry(telemetry_data):
        """Find bidirectional pairs in telemetry data, matching the algorithm logic"""
        bidirectional_pairs = {}
        
        for link_name in telemetry_data.keys():
            if '-' not in link_name:
                continue
                
            src, dst = link_name.split('-')
            reverse_link = f"{dst}-{src}"
            
            # Check if reverse link exists in telemetry
            if reverse_link in telemetry_data:
                # Create canonical name (alphabetically sorted)
                canonical_name = f"{min(src, dst)}-{max(src, dst)}"
                
                # Only add if not already processed
                if canonical_name not in bidirectional_pairs:
                    bidirectional_pairs[canonical_name] = (link_name, reverse_link)
        
        return bidirectional_pairs
    
    def count_closed_pairs(links_to_close, bidirectional_pairs):
        """Count how many bidirectional pairs are being closed"""
        closed_pairs = set()
        
        for link in links_to_close:
            if '-' not in link:
                continue
                
            src, dst = link.split('-')
            canonical_name = f"{min(src, dst)}-{max(src, dst)}"
            
            # Check if this canonical pair exists in our bidirectional pairs
            if canonical_name in bidirectional_pairs:
                closed_pairs.add(canonical_name)
        
        return len(closed_pairs)
    
    # Find all bidirectional pairs in telemetry data
    bidirectional_pairs = find_bidirectional_pairs_in_telemetry(telemetry_data)
    total_pairs = len(bidirectional_pairs)
    
    # Count how many pairs are being closed
    closed_pairs_count = count_closed_pairs(links_to_close, bidirectional_pairs)
    
    print(f"ğŸ”‹ Energy calculation: {closed_pairs_count} bidirectional pairs to close out of {total_pairs} total pairs")
    print(f"ğŸ”— Raw counts: {len(links_to_close)} directional links to close, {len(telemetry_data)} total telemetry links")
    print(f"ğŸ“Š Bidirectional pairs found: {list(bidirectional_pairs.keys())[:5]}{'...' if len(bidirectional_pairs) > 5 else ''}")
    
    if total_pairs > 0:
        base_energy_saving = closed_pairs_count / total_pairs
        energy_saving = min(1.0, base_energy_saving * 2)  # Double the percentage, cap at 100%
        print(f"ğŸ”‹ Energy saving: {energy_saving:.3f} ({energy_saving:.1%}) - {closed_pairs_count}/{total_pairs} pairs (doubled)")
    else:
        energy_saving = 0.0
        print(f"ğŸ”‹ No bidirectional pairs found, energy saving = 0%")
    
    # Process predicted links into RESTCONF commands
    file_commands, api_commands, configs, commands_file, config_files = process_predicted_links(links_to_close)
    print(f"ğŸ“ RESTCONF commands saved to: {commands_file}")
    ## 
    telemetry_json = json.dumps(telemetry_data, ensure_ascii=False)
    
    # Use custom prompt or default
    if user_prompt:
        prompt = user_prompt
    else:
        prompt = f"è«‹æ ¹æ“šä¸Šè¿°è³‡æ–™ï¼Œé—œé–‰ {', '.join(links_to_close)}ï¼Œæ•´ç†è¼¸å‡ºç›¸é—œåƒè€ƒè³‡æ–™"
    
    # Enhance prompt with RAG
    if rag_system is not None:
        try:
            enhanced_prompt = rag_system.enhance_prompt(prompt, SYSTEM_PROMPT)
            print(f"ğŸ“š RAG enhanced prompt with relevant documents")
        except Exception as e:
            print(f"âš ï¸ RAG enhancement failed: {e}")
            enhanced_prompt = prompt
            print(f"ğŸ“ Using original prompt without RAG")
    else:
        enhanced_prompt = prompt
        print(f"ğŸ“ RAG system not available, using original prompt")

    full_prompt = (
        f"ä»¥ä¸‹æ˜¯å³æ™‚æµé‡è³‡æ–™ï¼ˆJSONï¼‰ï¼š\n{telemetry_json}\n\n"
        f"{enhanced_prompt}"
    )

    chat = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user",   "content": full_prompt},
    ]

    input_ids = tokenizer.apply_chat_template(
        chat,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
    ).to("cuda")                           # ğŸ”§ å¼µé‡ä¹Ÿæ¬åˆ° GPU

    with torch.no_grad():
        outputs = model.generate(
            input_ids       = input_ids,
            max_new_tokens  = 256,
            do_sample       = True,
            temperature     = 0.7,
            top_p           = 0.9,
        )

    generated = outputs[0, input_ids.shape[-1]:]
    # Format commands without escape characters
    formatted_commands = []
    for cmd in api_commands:
        # Remove any remaining escape characters
        clean_cmd = cmd.replace('\\', '').replace('\n', ' ').replace('\'', '"')
        formatted_commands.append(clean_cmd)
    
    # Create detailed instructions with shutdown modification steps
    shutdown_instructions = f"ç¯€èƒ½è·¯å¾‘è¨­å®šæŒ‡ä»¤åƒè€ƒï¼Œè«‹æ ¹æ“šä¸Šè¿°è³‡æ–™ï¼Œé—œé–‰ {', '.join(links_to_close)}ï¼Œé—œé–‰çš„RESTCONFå‘½ä»¤ç‚º{formatted_commands}"

    final_instructions = """
    é‡è¦æ“ä½œæ­¥é©Ÿ
    1. åŸ·è¡Œ GET å‘½ä»¤ç²å–ç•¶å‰ä»‹é¢é…ç½®ä¸¦ä¿å­˜åˆ° JSON æª”æ¡ˆ
    2. ç·¨è¼¯ JSON æª”æ¡ˆï¼Œåœ¨ interface-configuration ä¸­æ–°å¢ "shutdown":[null] æ¬„ä½
    ç¯„ä¾‹ä¿®æ”¹å‰
    {{"interface-configuration":{{{"active":"act", "interface-name":"GigabitEthernet0/0/0/0", "Cisco-IOS-XR-ipv4-network":{{"addresses":{{"primary":{{"address":"10.0.4.1","netmask":"255.255.255.252"}}}}}}}}}}}
    
    ç¯„ä¾‹ä¿®æ”¹å¾Œ
    {{"interface-configuration":{{{"active":"act", "shutdown":[null], "interface-name":"GigabitEthernet0/0/0/0", "Cisco-IOS-XR-ipv4-network":{{"addresses":{{"primary":{{"address":"10.0.4.1","netmask":"255.255.255.252"}}}}}}}}}}}

    3. åŸ·è¡Œ PUT å‘½ä»¤å°‡ä¿®æ”¹å¾Œçš„é…ç½®å¥—ç”¨åˆ°è¨­å‚™
    """
    
    return tokenizer.decode(generated, skip_special_tokens=True).strip() + final_instructions , shutdown_instructions , energy_saving

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â¼ FastAPI ç«¯é» (åŸæ¨£) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = FastAPI(
    title       = "XR Telemetry + LLM Demo",
    description = "ç”¢ç”ŸTelemetryï¼Œä¸¦ç”¨ UnsLoTH Llama-3.1 taide ç”¢ RESTCONF æŒ‡ä»¤",
    version     = "1.0.0",
)

@app.get("/telemetry")
async def get_telemetry():
    try:
        # ç²å–åŸå§‹telemetryæ•¸æ“š
        raw_data = collector(use_rates=True, measurement_interval=10)
        
        # åªè¿”å›æ•´æ•¸å€¼
        formatted_data = {}
        for link, value in raw_data.items():
            if isinstance(value, (int, float)):
                formatted_data[link] = int(round(value))
            else:
                formatted_data[link] = value
        
        return formatted_data
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
@app.get("/input")
async def get_input():
    try:
        return {"input": "input"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

from pydantic import BaseModel
from typing import Optional

class OutputRequest(BaseModel):
    message: Optional[str] = None

def parse_protected_links(message: str) -> list:
    """Parse user message to extract protected links from brackets []
    
    Args:
        message: User message that may contain protected links like [S1-S4]
        
    Returns:
        list: List of protected link names (including bidirectional pairs)
    """
    if not message:
        return []
    
    # Find all content within square brackets
    bracket_matches = re.findall(r'\[([^\]]+)\]', message)
    protected_links = []
    
    for match in bracket_matches:
        # Split by comma or space to handle multiple links in one bracket
        links = re.split(r'[,\s]+', match.strip())
        for link in links:
            link = link.strip()
            if link and '-' in link:  # Basic validation for link format
                protected_links.append(link)
                
                # Add bidirectional link (S6-S15 -> also protect S15-S6)
                parts = link.split('-')
                if len(parts) == 2:
                    reverse_link = f"{parts[1]}-{parts[0]}"
                    if reverse_link not in protected_links:
                        protected_links.append(reverse_link)
    
    return protected_links

def filter_links_to_close(predicted_links: list, protected_links: list) -> list:
    """Filter out protected links from predicted links to close
    
    Args:
        predicted_links: Links predicted by the model to be closed
        protected_links: Links that user wants to keep open
        
    Returns:
        list: Filtered links excluding protected ones
    """
    if not protected_links:
        return predicted_links
    
    filtered_links = [link for link in predicted_links if link not in protected_links]
    
    if len(filtered_links) != len(predicted_links):
        excluded = [link for link in predicted_links if link in protected_links]
        print(f"ğŸ”’ Excluded protected links: {excluded}")
        print(f"ğŸ“‹ Filtered links to close: {filtered_links}")
    
    return filtered_links

def filter_links_by_topology(predicted_links: list) -> list:
    """
    Filter out links that contain nodes not present in the current topology
    
    Args:
        predicted_links: Links predicted by the model to be closed
        
    Returns:
        list: Filtered links excluding those with unopened nodes
    """
    try:
        # Get current topology to find available nodes
        from restconf_processor import fetch_all_nodes
        available_nodes = fetch_all_nodes()
        
        if not available_nodes:
            print("âš ï¸  No nodes found in topology, returning empty list")
            return []
        
        # Extract node numbers from available nodes (e.g., 'node1' -> 'S1')
        available_switches = set()
        for node_id in available_nodes:
            if 'node' in node_id:
                try:
                    node_num = node_id.replace('node', '')
                    switch_name = f"S{node_num}"
                    available_switches.add(switch_name)
                except ValueError:
                    continue
        
        print(f"ğŸ” Available switches in topology: {sorted(available_switches)}")
        
        # Filter links to only include those where both nodes are available
        valid_links = []
        for link in predicted_links:
            if '-' in link:
                try:
                    source, target = link.split('-', 1)
                    if source in available_switches and target in available_switches:
                        valid_links.append(link)
                    else:
                        print(f"ğŸš« Filtered out link {link}: {source} or {target} not in topology")
                except ValueError:
                    print(f"âš ï¸  Invalid link format: {link}")
            else:
                print(f"âš ï¸  Invalid link format: {link}")
        
        print(f"âœ… Topology filtering: {len(predicted_links)} -> {len(valid_links)} links")
        return valid_links
        
    except Exception as e:
        print(f"âš ï¸  Error filtering links by topology: {e}")
        print("   Returning original links as fallback")
        return predicted_links

@app.post("/output")
async def post_output(request: OutputRequest):
    """Process user message with full network analysis pipeline"""
    try:
        # Extract message and settings from request body
        user_prompt = request.message
        print(f"ğŸ“¨ Received user message: {user_prompt}")
        
        # Parse protected links from user message (includes bidirectional pairs)
        protected_links = parse_protected_links(user_prompt)
        if protected_links:
            print(f"ğŸ”’ Protected links found (including bidirectional): {protected_links}")
        
        # Get fresh telemetry data and predictions
        telemetry_data = collector()
        links_to_close = predict_links_to_close_rl(telemetry_data)
        
        # Filter out links to nodes not in current topology
        topology_filtered_links = filter_links_by_topology(links_to_close)
        
        # Filter out protected links from the prediction
        filtered_links_to_close = filter_links_to_close(topology_filtered_links, protected_links)
        
        # Run LLM inference with user message and filtered links
        # Pass the filtered links to ensure RESTCONF commands only include allowed links
        result, commands, energy_saving = llm_inference(user_prompt=user_prompt, predicted_links=filtered_links_to_close)
        
        return {
            "llm_result": result,
            "restconf_commands": commands,
            "energy_saving_percentage": f"{energy_saving:.1%}",
            "predicted_links_to_close": filtered_links_to_close,
            "protected_links": protected_links,
            "original_prediction": links_to_close,
            "topology_filtered_prediction": topology_filtered_links
        }
    except Exception as e:
        print(f"âŒ Error in POST /output: {e}")
        raise HTTPException(status_code=500, detail=str(e))
        
@app.get("/output")
async def get_output():
    """Get LLM inference result with network analysis"""
    try:
        # Get fresh telemetry data and predictions
        telemetry_data = collector()
        links_to_close = predict_links_to_close_rl(telemetry_data)
        
        # Filter out links to nodes not in current topology
        topology_filtered_links = filter_links_by_topology(links_to_close)
        
        # Run LLM inference with topology-filtered links
        result, commands, energy_saving = llm_inference(predicted_links=topology_filtered_links)
        
        return {
            "llm_result": result, 
            "restconf_commands": commands,
            "energy_saving_percentage": f"{energy_saving:.1%}",
            "predicted_links_to_close": topology_filtered_links,
            "original_prediction": links_to_close,
            "topology_filtered_prediction": topology_filtered_links
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# æ–°å¢ç«¯é»ï¼šä½¿ç”¨RLæ¨¡å‹é æ¸¬è¦é—œé–‰çš„link
@app.get("/predict-links-rl")
async def predict_links_rl():
    """ä½¿ç”¨RLæ¨¡å‹ä¾†é æ¸¬æ‡‰è©²é—œé–‰å“ªäº›link"""
    try:
        # Get raw telemetry data (simple float values)
        raw_telemetry = collector()
        
        # Convert to format expected by RL model
        telemetry_data = {}
        for link, traffic_value in raw_telemetry.items():
            if isinstance(traffic_value, (int, float)):
                telemetry_data[link] = {
                    'traffic': traffic_value,
                    'output-drops': 0,  # Simulated values since we don't have real drop data
                    'output-queue-drops': 0,
                    'max-capacity': 8000
                }
        
        links_to_close = predict_links_to_close_rl(telemetry_data)
        
        # Filter out links to nodes not in current topology
        topology_filtered_links = filter_links_by_topology(links_to_close)
        
        # Process topology-filtered links into RESTCONF commands
        file_commands, api_commands, configs, commands_file, config_files = process_predicted_links(topology_filtered_links)
        
        return {
            "telemetry_data": telemetry_data,
            "predicted_links_to_close": topology_filtered_links,
            "original_prediction": links_to_close,
            "topology_filtered_prediction": topology_filtered_links,
            "restconf_commands": api_commands,
            "commands_file": str(commands_file),
            "config_files": [str(f) for f in config_files],
            "model_info": rl_manager.get_model_info()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# æ–°å¢ç«¯é»ï¼šç²å–RLæ¨¡å‹ä¿¡æ¯
@app.get("/rl-model-info")
async def get_rl_model_info():
    """ç²å–RLæ¨¡å‹ä¿¡æ¯"""
    try:
        model_info = rl_manager.get_model_info()
        return model_info
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# æ–°å¢ç«¯é»ï¼šæŸ¥çœ‹telemetryæ•¸æ“šå¦‚ä½•è½‰æ›ç‚ºæ¨¡å‹ç‰¹å¾µ
@app.get("/telemetry-features")
async def get_telemetry_features():
    """ç²å–ç•¶å‰telemetryæ•¸æ“šåŠå…¶è½‰æ›ç‚ºæ¨¡å‹ç‰¹å¾µçš„è©³ç´°ä¿¡æ¯"""
    try:
        # æ”¶é›†ç•¶å‰telemetryæ•¸æ“š
        raw_telemetry = collector(use_rates=True, measurement_interval=5)
        
        # è½‰æ›ç‚ºæ¨¡å‹æœŸæœ›çš„æ ¼å¼
        telemetry_data = {}
        for link, traffic_value in raw_telemetry.items():
            if isinstance(traffic_value, (int, float)):
                telemetry_data[link] = {
                    'traffic': traffic_value,
                    'output-drops': 0,  # æ¨¡æ“¬å€¼
                    'output-queue-drops': 0,  # æ¨¡æ“¬å€¼
                    'max-capacity': 8000  # æ¨¡æ“¬å€¼
                }
        
        # ç²å–æ¨¡å‹çš„é è™•ç†ç‰¹å¾µ
        features = rl_manager.preprocess_telemetry_data(telemetry_data)
        
        # å‰µå»ºè©³ç´°çš„ç‰¹å¾µè§£é‡‹
        feature_details = []
        feature_names = [
            "buffer_utilization",
            "link_utilization", 
            "link_status",
            "buffer_change_rate",
            "util_change_rate",
            "time_since_change",
            "normalized_node_degree"
        ]
        
        for i, link in enumerate(rl_manager.links):
            link_features = {}
            
            # åŸå§‹telemetryæ•¸æ“š
            raw_data = telemetry_data.get(link, {})
            
            # æ¨¡å‹ç‰¹å¾µ
            if i < len(features):
                for j, feature_name in enumerate(feature_names):
                    link_features[feature_name] = float(features[i][j]) if j < len(features[i]) else 0.0
            
            # è¨ˆç®—è©³ç´°ä¿¡æ¯
            traffic = raw_data.get('traffic', 0)
            max_capacity = raw_data.get('max-capacity', 1000)
            output_drops = raw_data.get('output-drops', 0)
            output_queue_drops = raw_data.get('output-queue-drops', 0)
            total_drops = output_drops + output_queue_drops
            
            # è¨ˆç®—æ¨å°å€¼
            drop_rate = total_drops / max(1, traffic + total_drops) if (traffic + total_drops) > 0 else 0
            calculated_buffer_util = min(1.0, drop_rate * 10)
            calculated_link_util = min(1.0, traffic / max_capacity) if max_capacity > 0 else 0.0
            
            feature_details.append({
                "link": link,
                "raw_telemetry": {
                    "traffic_bytes_per_sec": traffic,
                    "max_capacity": max_capacity,
                    "output_drops": output_drops,
                    "output_queue_drops": output_queue_drops,
                    "total_drops": total_drops,
                    "drop_rate": drop_rate
                },
                "calculated_metrics": {
                    "buffer_utilization": calculated_buffer_util,
                    "link_utilization": calculated_link_util,
                    "utilization_percentage": calculated_link_util * 100
                },
                "model_features": link_features,
                "feature_explanations": {
                    "buffer_utilization": f"Estimated from drop rate: {drop_rate:.4f} * 10 = {calculated_buffer_util:.4f}",
                    "link_utilization": f"Traffic/Capacity: {traffic}/{max_capacity} = {calculated_link_util:.4f}",
                    "link_status": "1.0 = UP, 0.0 = DOWN (currently assumed UP)",
                    "buffer_change_rate": "Change in buffer utilization from previous measurement",
                    "util_change_rate": "Change in link utilization from previous measurement",
                    "time_since_change": "Time since last link state change (simplified to 0)",
                    "normalized_node_degree": "Node connectivity degree normalized to [0,1]"
                }
            })
        
        return {
            "timestamp": telemetry_data.get('timestamp', 'unknown'),
            "measurement_interval_seconds": 5,
            "total_links": len(rl_manager.links),
            "features_per_link": len(feature_names),
            "feature_names": feature_names,
            "model_input_shape": list(features.shape) if features is not None else None,
            "link_details": feature_details,
            "summary": {
                "avg_buffer_utilization": float(np.mean([f["model_features"]["buffer_utilization"] for f in feature_details])),
                "avg_link_utilization": float(np.mean([f["model_features"]["link_utilization"] for f in feature_details])),
                "max_link_utilization": float(np.max([f["model_features"]["link_utilization"] for f in feature_details])),
                "links_with_drops": len([f for f in feature_details if f["raw_telemetry"]["total_drops"] > 0]),
                "active_links": len([f for f in feature_details if f["raw_telemetry"]["traffic_bytes_per_sec"] > 0])
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting telemetry features: {str(e)}")

# æ–°å¢ç«¯é»ï¼šè¼‰å…¥æ–‡æª”åˆ°RAGç³»çµ±
@app.post("/load-document")
async def load_document(file_path: str, force_reload: bool = False):
    """è¼‰å…¥æ–‡æª”åˆ°RAGç³»çµ±"""
    try:
        rag_system.load_documents(file_path, force_reload=force_reload)
        return {
            "message": f"Document {file_path} loaded successfully",
            "document_info": rag_system.get_document_info()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# æ–°å¢ç«¯é»ï¼šç²å–RAGç³»çµ±ä¿¡æ¯
@app.get("/rag-info")
async def get_rag_info():
    """ç²å–RAGç³»çµ±ä¿¡æ¯"""
    try:
        return rag_system.get_document_info()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# æ–°å¢ç«¯é»ï¼šæœç´¢ç›¸é—œæ–‡æª”
@app.get("/search-documents")
async def search_documents(query: str, top_k: int = 3):
    """æœç´¢ç›¸é—œæ–‡æª”"""
    try:
        results = rag_system.retrieve_relevant_docs(query, top_k=top_k)
        return {
            "query": query,
            "results": results,
            "total_found": len(results)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# æ–°å¢ç«¯é»ï¼šç²å–æ‹“æ’²ä¿¡æ¯
@app.get("/topology-info")
async def get_topology_info():
    """ç²å–ç¶²è·¯æ‹“æ’²ä¿¡æ¯ï¼ŒåŒ…å«æ‰€æœ‰ç¯€é»å’Œä»‹é¢çš„çœŸå¯¦IPv4åœ°å€"""
    try:
        from restconf_processor import fetch_topology_info
        topology_data = fetch_topology_info()
        return {
            "topology_info": topology_data,
            "total_interfaces": len(topology_data),
            "message": "Complete topology information fetched successfully"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# æ–°å¢ç«¯é»ï¼šä¸‹è¼‰RESTCONFå‘½ä»¤æ–‡ä»¶
@app.get("/download-commands")
async def download_commands(filename: str = None):
    """ä¸‹è¼‰ç”Ÿæˆçš„RESTCONFå‘½ä»¤æ–‡ä»¶"""
    try:
        output_dir = Path("restconf_output")
        
        if filename:
            # Download specific file
            file_path = output_dir / filename
            if not file_path.exists():
                raise HTTPException(status_code=404, detail=f"File {filename} not found")
        else:
            # Download the most recent commands file
            command_files = list(output_dir.glob("restconf_commands_*.txt"))
            if not command_files:
                raise HTTPException(status_code=404, detail="No command files found")
            
            # Get the most recent file
            file_path = max(command_files, key=lambda f: f.stat().st_mtime)
        
        return FileResponse(
            path=str(file_path),
            filename=file_path.name,
            media_type="text/plain"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# æ–°å¢ç«¯é»ï¼šåˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„å‘½ä»¤æ–‡ä»¶
@app.get("/list-command-files")
async def list_command_files():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„RESTCONFå‘½ä»¤æ–‡ä»¶"""
    try:
        output_dir = Path("restconf_output")
        
        if not output_dir.exists():
            return {"files": [], "message": "No output directory found"}
        
        # Get all command files
        command_files = list(output_dir.glob("restconf_commands_*.txt"))
        config_files = list(output_dir.glob("config_*.json"))
        
        file_info = []
        
        # Add command files info
        for file_path in command_files:
            stat = file_path.stat()
            file_info.append({
                "filename": file_path.name,
                "type": "commands",
                "size": stat.st_size,
                "created": stat.st_mtime,
                "download_url": f"/download-commands?filename={file_path.name}"
            })
        
        # Add config files info
        for file_path in config_files:
            stat = file_path.stat()
            file_info.append({
                "filename": file_path.name,
                "type": "config",
                "size": stat.st_size,
                "created": stat.st_mtime,
                "download_url": f"/download-config?filename={file_path.name}"
            })
        
        # Sort by creation time (newest first)
        file_info.sort(key=lambda x: x["created"], reverse=True)
        
        return {
            "files": file_info,
            "total_files": len(file_info),
            "command_files": len(command_files),
            "config_files": len(config_files)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# æ–°å¢ç«¯é»ï¼šä¸‹è¼‰é…ç½®æ–‡ä»¶
@app.get("/download-config")
async def download_config(filename: str):
    """ä¸‹è¼‰ç”Ÿæˆçš„JSONé…ç½®æ–‡ä»¶"""
    try:
        output_dir = Path("restconf_output")
        file_path = output_dir / filename
        
        if not file_path.exists():
            raise HTTPException(status_code=404, detail=f"Config file {filename} not found")
        
        return FileResponse(
            path=str(file_path),
            filename=file_path.name,
            media_type="application/json"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# æ–°å¢ç«¯é»ï¼šåŸ·è¡Œå–®ä¸€é€£çµé—œé–‰å‘½ä»¤
@app.post("/close-link")
async def close_link(link: str):
    """Execute close command for a specific link"""
    try:
        # Validate link format
        if not link or '-' not in link:
            raise HTTPException(status_code=400, detail="Invalid link format. Expected format: S1-S2")
        
        # Apply topology filtering to ensure the link is valid
        topology_filtered_links = filter_links_by_topology([link])
        
        if not topology_filtered_links:
            raise HTTPException(
                status_code=400, 
                detail=f"Link {link} contains nodes not present in current topology"
            )
        
        # Build and execute the shutdown command
        commands = build_shutdown_commands([link], for_file=False)
        
        if not commands:
            raise HTTPException(
                status_code=400, 
                detail=f"No interface mapping found for link {link}"
            )
        
        # Execute the command using requests
        import subprocess
        import shlex
        from datetime import datetime
        
        command = commands[0]
        print(f"ğŸ”§ Executing close command for link {link}")
        print(f"ğŸ“ Command: {command}")
        
        # Execute the curl command
        try:
            # Parse the curl command to extract components
            if 'curl' in command:
                # Execute the command directly
                result = subprocess.run(
                    shlex.split(command), 
                    capture_output=True, 
                    text=True, 
                    timeout=30
                )
                
                if result.returncode == 0:
                    print(f"âœ… Successfully closed link {link}")
                    status = "success"
                    message = f"Link {link} has been successfully closed"
                    output = result.stdout
                else:
                    print(f"âŒ Failed to close link {link}: {result.stderr}")
                    status = "error"
                    message = f"Failed to close link {link}"
                    output = result.stderr
            else:
                raise ValueError("Invalid command format")
                
        except subprocess.TimeoutExpired:
            status = "timeout"
            message = f"Command timeout while closing link {link}"
            output = "Command execution timed out"
        except Exception as e:
            status = "error"
            message = f"Error executing command: {str(e)}"
            output = str(e)
        
        return {
            "link": link,
            "status": status,
            "message": message,
            "command": command,
            "output": output,
            "timestamp": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Error in close-link endpoint: {e}")
        raise HTTPException(status_code=500, detail=str(e))
